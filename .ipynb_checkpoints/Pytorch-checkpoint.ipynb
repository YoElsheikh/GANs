{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59efb2d",
   "metadata": {},
   "source": [
    "## Data pereparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f455ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets #dataset within Pytorch\n",
    "                                            #transforms is what we will apply to the data\n",
    "import torch.utils.data as tud\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f010f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True,   #first spot specifies where we want the data to go \"\" means we want it to go locally\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))  #here we use the \n",
    "#transform to tensor because the MNIST data isn't in tensor format\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                     transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "156e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = tud.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = tud.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8717011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([2, 3, 2, 3, 5, 3, 5, 0, 3, 7])]\n"
     ]
    }
   ],
   "source": [
    "#iterating over one batch of ten samples using break to stop at the first batch\n",
    "#labels at \"tensor\"\n",
    "\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9a52f6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]   #data[0th tensor of tensors, 0th element of that tensor] \n",
    "                                #so data[0][something] refers to the images and data[1][something]\n",
    "                                #refers to the labels\n",
    "\n",
    "#print(data[0][0])     \n",
    "print(data[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d55b2eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ef053f0100>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to plot the image with plt.imshow we need to get rid of the third grey scale dimention, \n",
    "#i.e. reshaping with view\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3632b",
   "metadata": {},
   "source": [
    "## Balancing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7746b",
   "metadata": {},
   "source": [
    "Motiation: is that the presece of a prominent feature will force the model to overfit to that\n",
    "dominant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d6495a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "#checking the samples for each label\n",
    "\n",
    "tot = 0\n",
    "counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    samples, labels = data \n",
    "    for i in labels:\n",
    "        counter[int(i)]+= 1\n",
    "        tot+=1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dcf859cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "for i in counter:  #looping through the elements in counter   \n",
    "    print(f\"{i}: {counter[i]/tot*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c19d6",
   "metadata": {},
   "source": [
    "data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c95c56",
   "metadata": {},
   "source": [
    "## Building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b86c5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  #the OOP (initializing parameters)\n",
    "import torch.nn.functional as F #funtions, passing parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9056865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#after defining the class, we will define in the (__init__) method the fully connected  layers\n",
    "#and in another method, we will define how data passes\n",
    "\n",
    "class Net(nn.Module):  #class inhering attributes from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#Return a proxy object that delegates method calls to a parent or sibling class of type. \n",
    "#This is useful for accessing inherited methods that have been overridden in a class.\n",
    "#see Python documentation: https://docs.python.org/2/library/functions.html#super\n",
    "        self.fc1 = nn.Linear(784, 64) #flattened 28*28 #syntax: self.\"name\" = nn. Linear(input, output)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        \n",
    "        #defining the feedforward\n",
    "    def forward(self, x):    #x is the input\n",
    "        x = F.relu(self.fc1(x)) #running relu element-wise over fc1\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) #applying softmax on the last layer, distributing \n",
    "                                       #about the output tensor from layer 4 that is of \n",
    "                                       #dimention = 1\n",
    "\n",
    "        \n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8e0f9244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3679, -2.3431, -2.3263, -2.4367, -2.2887, -2.2855, -2.1762, -2.2069,\n",
       "         -2.3502, -2.2713]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing random pixel data\n",
    "X = torch.rand((28,28))\n",
    "X = X.view(1,28*28)\n",
    "output = net(X)\n",
    "#X\n",
    "output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c2a51",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d59b00d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9709166666666667\n",
      "loss:  tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "Accuracy:  0.9719833333333333\n",
      "loss:  tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "Accuracy:  0.972575\n",
      "loss:  tensor(0.0263, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "num__of_epochs = 3\n",
    "\n",
    "for epoch in range(num__of_epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad() #In PyTorch, we need to set the gradients to zero before starting to do backpropragation because\n",
    "                        #PyTorch accumulates the gradients on subsequent backward passes.\n",
    "                        #This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) \n",
    "                        #the gradients on every loss.backward() call.\n",
    "\n",
    "                        #Because of this, when you start your training loop, \n",
    "                        #ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "                        #Else the gradient would point in some other direction than the \n",
    "                        #intended direction towards the minimum (or maximum, in case of maximization objectives).\n",
    "                        \n",
    "        output = net(X.view(-1, 28*28))                     \n",
    "        loss = F.nll_loss(output, y)                 \n",
    "        loss.backward()\n",
    "        optimizer.step() #adjusting the weights\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in trainset:\n",
    "            X, y = data\n",
    "            output = net(X.view(-1, 28*28))  #network is already trained and weights are adjusted after epoching \n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    print (\"Accuracy: \", correct/total)\n",
    "        \n",
    "        \n",
    "    print (\"loss: \", loss)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa74f82",
   "metadata": {},
   "source": [
    "## Evaluating the model after 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "20212798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.97435\n"
     ]
    }
   ],
   "source": [
    "#for/after the 3 epochs, i.e. for the final model, check above for individual epochs\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))  #network is already trained and weights are adjusted after epoching \n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print (\"Accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "16313645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[5].view(28,28))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "57792200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[5].view(-1,28*28))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "82b6ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data in trainset:\n",
    "    #X, y = data\n",
    "    #print(X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
