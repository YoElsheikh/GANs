{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978f56a4",
   "metadata": {},
   "source": [
    "## Data pereparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6aa9f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets #dataset within Pytorch\n",
    "                                            #transforms is what we will apply to the data\n",
    "import torch.utils.data as tud\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1abe58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True,   #first spot specifies where we want the data to go \"\" means we want it to go locally\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))  #here we use the \n",
    "#transform to tensor because the MNIST data isn't in tensor format\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                     transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bb864286",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = tud.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = tud.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f9974025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([5, 5, 0, 0, 2, 3, 5, 0, 2, 0])]\n"
     ]
    }
   ],
   "source": [
    "#iterating over one batch of ten samples using break to stop at the first batch\n",
    "#labels at \"tensor\"\n",
    "\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "53aa7484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]   #data[0th tensor of tensors, 0th element of that tensor] \n",
    "                                #so data[0][something] refers to the images and data[1][something]\n",
    "                                #refers to the labels\n",
    "\n",
    "#print(data[0][0])     \n",
    "print(data[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "256bc644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ef067769a0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to plot the image with plt.imshow we need to get rid of the third grey scale dimention, \n",
    "#i.e. reshaping with view\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174633f1",
   "metadata": {},
   "source": [
    "## Balancing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c15b4b",
   "metadata": {},
   "source": [
    "Motiation: is that the presece of a prominent feature will force the model to overfit to that\n",
    "dominant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0253a227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "#checking the samples for each label\n",
    "\n",
    "tot = 0\n",
    "counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    samples, labels = data \n",
    "    for i in labels:\n",
    "        counter[int(i)]+= 1\n",
    "        tot+=1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1f02f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "for i in counter:  #looping through the elements in counter   \n",
    "    print(f\"{i}: {counter[i]/tot*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a68ca8",
   "metadata": {},
   "source": [
    "data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9c429",
   "metadata": {},
   "source": [
    "## Building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "819856bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  #the OOP (initializing parameters)\n",
    "import torch.nn.functional as F #funtions, passing parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6a6c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#after defining the class, we will define in the (__init__) method the fully connected  layers\n",
    "#and in another method, we will define how data passes\n",
    "\n",
    "class Net(nn.Module):  #class inhering attributes from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#Return a proxy object that delegates method calls to a parent or sibling class of type. \n",
    "#This is useful for accessing inherited methods that have been overridden in a class.\n",
    "#see Python documentation: https://docs.python.org/2/library/functions.html#super\n",
    "        self.fc1 = nn.Linear(784, 64) #flattened 28*28 #syntax: self.\"name\" = nn. Linear(input, output)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        \n",
    "        #defining the feedforward\n",
    "    def forward(self, x):    #x is the input\n",
    "        x = F.relu(self.fc1(x)) #running relu element-wise over fc1\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) #applying softmax on the last layer, distributing \n",
    "                                       #about the output tensor from layer 4 that is of \n",
    "                                       #dimention = 1\n",
    "\n",
    "        \n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "229ac13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2504, -2.4449, -2.2329, -2.3325, -2.2362, -2.2476, -2.3953, -2.3362,\n",
       "         -2.3474, -2.2288]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing random pixel data\n",
    "X = torch.rand((28,28))\n",
    "X = X.view(1,28*28)\n",
    "output = net(X)\n",
    "#X\n",
    "output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0de1a8",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad() #In PyTorch, we need to set the gradients to zero before starting to do backpropragation because\n",
    "                        #PyTorch accumulates the gradients on subsequent backward passes.\n",
    "                        #This is convenient while training RNNs. So, the default action is to accumulate (i.e. sum) \n",
    "                        #the gradients on every loss.backward() call.\n",
    "\n",
    "                        #Because of this, when you start your training loop, \n",
    "                        #ideally you should zero out the gradients so that you do the parameter update correctly.\n",
    "                        #Else the gradient would point in some other direction than the \n",
    "                        #intended direction towards the minimum (or maximum, in case of maximization objectives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
