{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a052b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.utils.data as tud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable \n",
    "# Variables wrap a Tensor\n",
    "#x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "# Variable containing:\n",
    "# 1  1\n",
    "# 1  1\n",
    "# [torch.FloatTensor of size 2x2]\n",
    "\n",
    "import torch.optim as optim #Optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#gpu if cuda exists, else run on cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98da666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True,\n",
    "                     transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = tud.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = tud.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479effc2",
   "metadata": {},
   "source": [
    "## Discriminator and Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c977194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):  \n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(img_dim, 128) #flattened 28*28 #syntax: self.\"name\" = nn. Linear(input, output)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):   #not taking any input \"images\", so x \n",
    "        x = F.leaky_relu(self.fc1(x), 0.1) #running leaky_relu element-wise over fc1, hp = 0.1\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(noise_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, img_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.1)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.tanh(x)\n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b18f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 64\n",
    "img_dim = 28*28\n",
    "\n",
    "\n",
    "Disc = Discriminator(img_dim = img_dim).to(device)\n",
    "Gen = Generator(noise_dim = noise_dim, img_dim = img_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad8a095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a9597a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Disc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee29eb",
   "metadata": {},
   "source": [
    "## Loss + Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e9bf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, doc. @ https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "criterion = nn.BCELoss()  #synt: criterion(input, target)\n",
    "\n",
    "#optimizer\n",
    "lr = 3e-4\n",
    "\n",
    "Disc_optimizer = optim.Adam(Disc.parameters(), lr = lr)\n",
    "Gen_optimizer = optim.Adam(Gen.parameters(), lr = lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18170c90",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dicriminator:\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "def Disc_train(x):\n",
    "    Disc.zero_grad()\n",
    "    \n",
    "    #using real data:\n",
    "    x_real, y_real = x.view(-1, 28*28), torch.ones(batch_size, 1)\n",
    "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
    "    \n",
    "    Disc_output = Disc(x_real) #output of the disctiminator's network\n",
    "    D_real_loss = criterion(D_output, y_real) #disc is a binary classifier, \n",
    "                                              #check documentation of BCELoss with y\n",
    "                                              #i think 1 for real data and 0 target for fake\n",
    "                                              #see below\n",
    "    D_real_score = Disc_output\n",
    "    \n",
    "    #using Generator (fake) data for the training:\n",
    "    \n",
    "    z = Variable(torch.randn(batch_size, noise_dim).to(device)) #100 batches for every Generator input\n",
    "    x_fake, y_fake = Gen(z), Variable(torch.zeros(batch_size, 1).to(device))\n",
    "    \n",
    "    Disc_output = Disc(x_fake)\n",
    "    D_fake_loss = criterion(Disc_output, y_fake)\n",
    "    D_fake_loss = Disc_output\n",
    "    \n",
    "    #originally we want to maximize and for the BCELoss, there's a minus sign indicating \n",
    "    #minimizing that same loss for the disc = maximizing the original loss in the paper\n",
    "    \n",
    "    \n",
    "    #backpropagating\n",
    "    D_loss = (D_real_loss + D_fake_loss)/2\n",
    "    D_loss.backward()\n",
    "    Disc_optimizer.step()\n",
    "    \n",
    "    return D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062acc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the Generator:\n",
    "\n",
    "Gen.zero_grad()\n",
    "\n",
    "z = Variable(torch.randn(batch_size, noise_dim))\n",
    "y = Varable(torch.zeros(batch_size, 1).to(device))\n",
    "\n",
    "G_output = Gen(z) # z --> Gen --> Disc(Gen) = D_output\n",
    "D_output = Disc(G_output)\n",
    "G_loss = criterion(D_output, y)\n",
    "\n",
    "#backpropagating\n",
    "G_loss.backward()\n",
    "Gen_optimizer.step()\n",
    "\n",
    "return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff1b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0c079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e1aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
