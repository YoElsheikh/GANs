{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfe82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#defining image dimentions\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels =  1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "#####################################################\n",
    "\n",
    "\n",
    "#given an input of noise (latent vector) vector, the Generator produces some image\n",
    "def build_generatr9():\n",
    "    \n",
    "    noise_shape = (100, 1) #this is the input latent (noise) vector, \n",
    "                           #which is a one dimentional array of size 100\n",
    "        \n",
    "#actually building the generator network, here using Dense layers, but the network can\n",
    "#change based on the application\n",
    "\n",
    "mdoel = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_shape=noise_shape))\n",
    "mode.add(LeakyReLU(alpha=0.2)) #the use of Leaky_Relu come in hand when the \n",
    "                               #network (using RelU as an activation fuinction) doen't \n",
    "                               #learn due to the fact that RelU is max[0,x] \n",
    "                               #so the negative values are set as zero, resulting in a \n",
    "                               #zero gradient and no learning Leaky ReLU is something like\n",
    "                               #max[0.1*x, x]\n",
    "                               #alpha is a hyper parameter\n",
    "                        \n",
    "model.add(BatchNormalization(momentum=0.8)) #BatchNormalization is a way of normalizing the data\n",
    "#between the layers, since we usually normalize the data before training, that is, normalized inputs\n",
    "#BatchNormalization is normalizing between the layers, i.e. the output of a chosen layer\n",
    "#this is set by subtracting the mean of the data from each datapoint and / std deviation\n",
    "# \n",
    "#momentum indicates the speed of the trainig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(BatchNormalization(momentum=0.8)) \n",
    "model.add(Dense(1024))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(BatchNormalization(momentum=0.8)) \n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(np.prod(img_shape), activation='tanh')\n",
    "model.add(Reshape(img_shape))\n",
    "          \n",
    "model.summary()\n",
    "          \n",
    "moise = Input(shape=noise_shape)\n",
    "img = model(noise)  #this is the Generated fake image\n",
    "          \n",
    "return Model(noise, img)\n",
    "          \n",
    "#model takes in random 1D image (of 100 entries) and generates a fake 2D image of 28x28\n",
    "          \n",
    "#############################################################\n",
    "          \n",
    "#here we have the discriminator, which is basically a binary classifier,  real or fake\n",
    "          \n",
    "          \n",
    "def build_discriminator():\n",
    "          \n",
    "model = Sequantial()\n",
    "model.add(Flatten(input_shape=img_shape))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summmary()\n",
    "          \n",
    "img = Input(shape=img_shape)\n",
    "validity = model(img)\n",
    "          \n",
    "return Model(validity, img)  #validity is the score, i.e. the probability of real vs fake \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7ce6c",
   "metadata": {},
   "source": [
    "## Process of BatchNormalization\n",
    "\n",
    "takes place within layers unlike usual normalization\n",
    "\n",
    "1. Normalize the output from activation function\n",
    "\n",
    "$z = (x - m)/s$ \n",
    "\n",
    "where, $m$ is the mean $s$ is the std deviation $x$ is the activation output\n",
    "\n",
    "2. Multiply normalized output with an arbitrary parameter \"$g$\"\n",
    "\n",
    "$z * g$\n",
    "\n",
    "3. Add arbitrary parameter, b, to the resulting product \n",
    "\n",
    "$(z * g) + b$\n",
    "\n",
    "the parameters $g$ and $b$ are tarinable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab957ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the models are now reay, and only in need of being put aginst each other. \n",
    "#this is done by defining a training function, loading the data set, reshaping our training\n",
    "#image andsetting the ground truths\n",
    "def train(epochs, batch_ssize=128, save_interval=200):\n",
    "    \n",
    "#lodding the dataset\n",
    "(X_train, _), (_, _) = mnist.lad_data()\n",
    "\n",
    "#converting to floating numebrs and rescaling the values between 1 and -1 (e.g. or 0 and 1)\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "#adding input channels since our input to both the gen and discr is 28x28x1\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "half_batch = int(batch_size / 2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
